# Tutorial: Redes Neurais Artificiais do Zero

### Baseado na obra de David Kopec \| Implementa√ß√£o em Python

Este projeto √© uma implementa√ß√£o did√°tica e pura em Python de uma **Rede Neural Artificial (ANN)** do zero, sem o uso de _frameworks_ de aprendizado de m√°quina como TensorFlow ou PyTorch. O objetivo √© focar nos **fundamentos matem√°ticos** e na **arquitetura de c√≥digo** que regem o funcionamento de uma rede neural.

Este guia serve como material de apoio para estudantes e desenvolvedores que desejam entender a implementa√ß√£o de uma **Rede Neural Artificial (ANN)** constru√≠da puramente em Python, focando nos fundamentos matem√°ticos, estat√≠sticos e na arquitetura de c√≥digo, sem o uso de frameworks externos.

A implementa√ß√£o √© baseada nos conceitos apresentados no livro *Classic Computer Science Problems* de David Kopec.

------------------------------------------------------------------------

## üìã Sum√°rio

1.  [Arquitetura e Componentes](#1-arquitetura-e-componentes)
    *   [1.1. O Neur√¥nio (`Neuron`)](#11-o-neur√¥nio-neuron)
    *   [1.2. A Camada (`Layer`)](#12-a-camada-layer)
    *   [1.3. A Rede (`Network`)](#13-a-rede-network)
2.  [Matem√°tica e Fun√ß√µes Auxiliares](#2-matem√°tica-e-fun√ß√µes-auxiliares)
3.  [Treinamento e Aprendizado](#3-treinamento-e-aprendizado)
    *   [3.1. Feedforward](#31-feedforward)
    *   [3.2. Backpropagation](#32-backpropagation)
4.  [Estrutura do C√≥digo](#4-estrutura-do-c√≥digo)
5.  [Como Executar os Exemplos](#5-como-executar-os-exemplos)
6.  [Refer√™ncias](#6-refer√™ncias)

---

## 1. Arquitetura e Componentes

O projeto segue uma arquitetura modular, onde cada elemento fundamental da rede neural √© representado por uma classe ou m√≥dulo Python.

### 1.1. O Neur√¥nio (`Neuron`)

Cada neur√¥nio recebe dados, processa-os e decide o quanto "disparar" de sinal para o pr√≥ximo.
![Rede neural biol√≥gica e artificial](./img/neurons.jpeg)

A unidade b√°sica de processamento. Cada neur√¥nio √© respons√°vel por:
*   Armazenar seus **pesos** (`weights`) e **vi√©s** (impl√≠cito no c√°lculo do produto escalar com um vi√©s de entrada, ou como parte dos pesos, dependendo da implementa√ß√£o exata, mas o c√≥digo foca em pesos e um `output_cache`).
*   Realizar a **combina√ß√£o linear** (produto escalar) das entradas com seus pesos.
*   Aplicar a **fun√ß√£o de ativa√ß√£o** (Sigmoide) ao resultado.
*   Armazenar o **delta** de erro para a fase de *backpropagation*.



### 1.2. A Camada (`Layer`)

Uma camada √© uma cole√ß√£o de neur√¥nios que operam em paralelo. A classe `Layer` gerencia:
*   A cria√ß√£o e inicializa√ß√£o dos neur√¥nios.
*   A propaga√ß√£o dos sinais de entrada para todos os neur√¥nios da camada.
*   O c√°lculo dos deltas de erro para a fase de *backpropagation* (m√©todos `calculate_deltas_for_output_layer` e `calculate_deltas_for_hidden_layer`).

### 1.3. A Rede (`Network`)

A classe `Network` √© o orquestrador principal, respons√°vel por:
*   Definir a **estrutura da rede** (n√∫mero de camadas e neur√¥nios em cada uma).
*   Gerenciar a sequ√™ncia de camadas.
*   Implementar os m√©todos de **treinamento** (`train`) e **valida√ß√£o** (`validate`).
*   Propagar o sinal de entrada atrav√©s de todas as camadas (`outputs`).

## 2. Matem√°tica e Fun√ß√µes Auxiliares

O m√≥dulo `util.py` cont√©m as fun√ß√µes matem√°ticas essenciais para o funcionamento da rede:

| Fun√ß√£o | Descri√ß√£o | F√≥rmula Matem√°tica |
| :--- | :--- | :--- |
| `dot_product` | Calcula o produto escalar entre dois vetores (entradas e pesos). | $z = \sum_{i} (x_i \cdot w_i)$ |
| `sigmoid` | Fun√ß√£o de ativa√ß√£o utilizada para "esmagar" o valor de sa√≠da do neur√¥nio para um intervalo entre 0 e 1. | $S(x) = \frac{1}{1 + e^{-x}}$ |
| `derivative_sigmoid` | Derivada da fun√ß√£o Sigmoide, crucial para o c√°lculo do gradiente no *backpropagation*. | $S'(x) = S(x) \cdot (1 - S(x))$ |
| `normalize_by_feature_scaling` | Fun√ß√£o de pr√©-processamento de dados que normaliza cada coluna do *dataset* para o intervalo $[0, 1]$. | $X_{norm} = \frac{X - X_{min}}{X_{max} - X_{min}}$ |

## 3. Treinamento e Aprendizado

O aprendizado da rede √© baseado no algoritmo de **Backpropagation**, que ajusta os pesos para minimizar o erro entre a sa√≠da prevista e a sa√≠da esperada.

### 3.1. Feedforward

A fase de propaga√ß√£o direta (`outputs` em `Network`) √© o processo de levar a entrada atrav√©s da rede, camada por camada, at√© a sa√≠da final. O resultado de uma camada serve como entrada para a pr√≥xima.

### 3.2. Backpropagation

A fase de propaga√ß√£o reversa (`backpropagate` em `Network`) √© o cora√ß√£o do aprendizado e envolve os seguintes passos:

1.  **C√°lculo do Erro:** O erro √© calculado na camada de sa√≠da.
2.  **Propaga√ß√£o do Erro:** O erro √© propagado de volta, da camada de sa√≠da para as camadas ocultas.
3.  **C√°lculo dos Deltas:** Para cada neur√¥nio, √© calculado o **delta** (a contribui√ß√£o do neur√¥nio para o erro total), usando a derivada da fun√ß√£o de ativa√ß√£o.
4.  **Atualiza√ß√£o dos Pesos:** Os pesos s√£o ajustados na dire√ß√£o oposta ao gradiente do erro, multiplicados pela **Taxa de Aprendizado** (`learning_rate`).

$$w_{novo} = w_{antigo} + (\text{learning\_rate} \cdot \text{delta} \cdot \text{output\_cache})$$

## 4. Estrutura do C√≥digo

O projeto est√° organizado da seguinte forma:

\`\`\`
ann/
‚îú‚îÄ‚îÄ Core/   
‚îÇ   ‚îú‚îÄ‚îÄ layer.py      # Implementa a Camada
‚îÇ   ‚îú‚îÄ‚îÄ network.py    # Implementa a Rede Neural (Orquestrador)
‚îÇ   ‚îú‚îÄ‚îÄ neuron.py     # Implementa o Neur√¥nio
‚îÇ   ‚îî‚îÄ‚îÄ util.py       # Fun√ß√µes matem√°ticas e de pr√©-processamento
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îú‚îÄ‚îÄ iris_test.py  # Exemplo de classifica√ß√£o do dataset Iris
‚îÇ   ‚îî‚îÄ‚îÄ wine_test.py  # Exemplo de classifica√ß√£o do dataset Wine
‚îî‚îÄ‚îÄ readme.md         # O README original (ser√° substitu√≠do)
\`\`\`

## 5. Como Executar os Exemplos

Para testar a rede neural, voc√™ pode executar os exemplos de classifica√ß√£o inclusos:

1.  **Pr√©-requisitos:** Certifique-se de ter o Python e as libs necess√°rias instaladas, estas √∫ltimas se encontram no arquivo requirements.txt. O projeto utiliza apenas bibliotecas padr√£o (como `csv` e `typing`). √â aconselh√°vel ter um ambiente Python para este projeto. A cria√ß√£o √© simples e usa os seguintes comandos:

\`\`\`bash
mkdir ~/envs
python3 -m venv ~/envs/positron
source ~/envs/positron/bin/activate
\`\`\`

e para instalar pacotes b√°sicos use os comandos:

\`\`\`bash
pip3 install --upgrade pip
pip3 install numpy pandas matplotlib seaborn scikit-learn
\`\`\`

2.  **Execu√ß√£o:** Navegue at√© a pasta `ann/examples` e execute o arquivo desejado.

**Exemplo: Classifica√ß√£o do Dataset Iris**

\`\`\`bash
cd ~/python/ANN   # diret√≥rio que cont√©m a pasta 'ann'
~/envs/positron/bin/python -m ann.examples.iris_test
\`\`\`

O script `iris_test.py` carrega o dataset Iris, normaliza os dados, treina a rede com 140 amostras por 50 √©pocas e testa a acur√°cia com as 10 amostras restantes.

## 6. Refer√™ncias

Este projeto √© uma implementa√ß√£o pr√°tica dos conceitos de redes neurais multicamadas com backpropagation, inspirada no trabalho de:

*   **[Kopec, David.](https://classicproblems.com/)** *Deep Learning from Scratch: Building with Python from First Principles*. Manning Publications, 2019. (Refer√™ncia principal para a arquitetura e l√≥gica de implementa√ß√£o).